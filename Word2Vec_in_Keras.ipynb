{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vec in Keras",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mhuckvale/pals0039/blob/master/Word2Vec_in_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztu-MXZvZ9q_",
        "colab_type": "text"
      },
      "source": [
        "#Word2vec training using Keras\n",
        "Adapted from [Adventures in Machine Learning](https://adventuresinmachinelearning.com/word2vec-keras-tutorial/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OqvXQ2sZjhI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import urllib\n",
        "import collections\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Reshape, dot\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "\n",
        "def maybe_download(filename, url, expected_bytes):\n",
        "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
        "    if not os.path.exists(filename):\n",
        "        filename, _ = urllib.request.urlretrieve(url + filename, filename)\n",
        "    statinfo = os.stat(filename)\n",
        "    if statinfo.st_size == expected_bytes:\n",
        "        print('Found and verified', filename)\n",
        "    else:\n",
        "        print(statinfo.st_size)\n",
        "        raise Exception(\n",
        "            'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
        "    return filename\n",
        "\n",
        "\n",
        "# Read the data into a list of strings.\n",
        "def read_data(filename):\n",
        "    \"\"\"Extract the first file enclosed in a zip file as a list of words.\"\"\"\n",
        "    with zipfile.ZipFile(filename) as f:\n",
        "        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
        "    return data\n",
        "\n",
        "\n",
        "def build_dataset(words, n_words):\n",
        "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
        "    count = [['UNK', -1]]\n",
        "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
        "    dictionary = dict()\n",
        "    for word, _ in count:\n",
        "        dictionary[word] = len(dictionary)\n",
        "    data = list()\n",
        "    unk_count = 0\n",
        "    for word in words:\n",
        "        if word in dictionary:\n",
        "            index = dictionary[word]\n",
        "        else:\n",
        "            index = 0  # dictionary['UNK']\n",
        "            unk_count += 1\n",
        "        data.append(index)\n",
        "    count[0][1] = unk_count\n",
        "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
        "    return data, count, dictionary, reversed_dictionary\n",
        "\n",
        "def collect_data(vocabulary_size=10000):\n",
        "    url = 'http://mattmahoney.net/dc/'\n",
        "    filename = maybe_download('text8.zip', url, 31344016)\n",
        "    vocabulary = read_data(filename)\n",
        "    print(vocabulary[:7])\n",
        "    data, count, dictionary, reverse_dictionary = build_dataset(vocabulary,\n",
        "                                                                vocabulary_size)\n",
        "    del vocabulary  # Hint to reduce memory.\n",
        "    return data, count, dictionary, reverse_dictionary\n",
        "\n",
        "vocab_size = 10000\n",
        "data, count, dictionary, reverse_dictionary = collect_data(vocabulary_size=vocab_size)\n",
        "print(data[:7])\n",
        "\n",
        "window_size = 3\n",
        "vector_dim = 300\n",
        "epochs = 200000\n",
        "\n",
        "valid_size = 16     # Random set of words to evaluate similarity on.\n",
        "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
        "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
        "\n",
        "print(\"Building skip grams\")\n",
        "sampling_table = sequence.make_sampling_table(vocab_size)\n",
        "couples, labels = skipgrams(data, vocab_size, window_size=window_size, sampling_table=sampling_table)\n",
        "word_target, word_context = zip(*couples)\n",
        "word_target = np.array(word_target, dtype=\"int32\")\n",
        "word_context = np.array(word_context, dtype=\"int32\")\n",
        "\n",
        "print(couples[:10], labels[:10])\n",
        "\n",
        "# create some input variables\n",
        "input_target = Input((1,))\n",
        "input_context = Input((1,))\n",
        "\n",
        "embedding = Embedding(vocab_size, vector_dim, input_length=1, name='embedding')\n",
        "target = embedding(input_target)\n",
        "target = Reshape((vector_dim, 1))(target)\n",
        "context = embedding(input_context)\n",
        "context = Reshape((vector_dim, 1))(context)\n",
        "\n",
        "# setup a cosine similarity operation which will be output in a secondary model\n",
        "similarity = dot([target, context], axes=0, normalize=True)\n",
        "\n",
        "# now perform the dot product operation to get a similarity measure\n",
        "dot_product = dot([target, context], axes=1, normalize=False)\n",
        "dot_product = Reshape((1,))(dot_product)\n",
        "# add the sigmoid output layer\n",
        "output = Dense(1, activation='sigmoid')(dot_product)\n",
        "# create the primary training model\n",
        "model = Model(inputs=[input_target, input_context], outputs=output)\n",
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop')\n",
        "\n",
        "# create a secondary validation model to run our similarity checks during training\n",
        "validation_model = Model(inputs=[input_target, input_context], outputs=similarity)\n",
        "\n",
        "\n",
        "class SimilarityCallback:\n",
        "    def run_sim(self):\n",
        "        for i in range(valid_size):\n",
        "            valid_word = reverse_dictionary[valid_examples[i]]\n",
        "            top_k = 8  # number of nearest neighbors\n",
        "            sim = self._get_sim(valid_examples[i])\n",
        "            nearest = (-sim).argsort()[1:top_k + 1]\n",
        "            log_str = 'Nearest to %s:' % valid_word\n",
        "            for k in range(top_k):\n",
        "                close_word = reverse_dictionary[nearest[k]]\n",
        "                log_str = '%s %s,' % (log_str, close_word)\n",
        "            print(log_str)\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_sim(valid_word_idx):\n",
        "        sim = np.zeros((vocab_size,))\n",
        "        in_arr1 = np.zeros((1,))\n",
        "        in_arr2 = np.zeros((1,))\n",
        "        in_arr1[0,] = valid_word_idx\n",
        "        for i in range(vocab_size):\n",
        "            in_arr2[0,] = i\n",
        "            out = validation_model.predict_on_batch([in_arr1, in_arr2])\n",
        "            sim[i] = out\n",
        "        return sim\n",
        "sim_cb = SimilarityCallback()\n",
        "\n",
        "arr_1 = np.zeros((1,))\n",
        "arr_2 = np.zeros((1,))\n",
        "arr_3 = np.zeros((1,))\n",
        "for cnt in range(epochs):\n",
        "    idx = np.random.randint(0, len(labels)-1)\n",
        "    arr_1[0,] = word_target[idx]\n",
        "    arr_2[0,] = word_context[idx]\n",
        "    arr_3[0,] = labels[idx]\n",
        "    loss = model.train_on_batch([arr_1, arr_2], arr_3)\n",
        "    if cnt % 100 == 0:\n",
        "        print(\"Iteration {}, loss={}\".format(cnt, loss))\n",
        "    if cnt % 10000 == 0:\n",
        "        sim_cb.run_sim()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}